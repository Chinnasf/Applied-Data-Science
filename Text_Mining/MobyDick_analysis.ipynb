{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date of Creation**: Jul 21, 2020<br>\n",
    "**Author**: Karina ChiÃ±as Fuentes\n",
    "\n",
    "---\n",
    "\n",
    "# Basic usages of `nltk`\n",
    "\n",
    "This notebook will show 11 diffent ways to use `nltk`'s functions to study the basic components of text, in this case, it will be part of the Moby Dick novel.\n",
    "\n",
    "1. Total of tokens\n",
    "2. Total of unique tokens\n",
    "3. Change in unique tokens after lemmatizing the verbs\n",
    "4. Lexical diversity of the text (i.e. ratio of unique tokens to the total number of tokens)\n",
    "5. Comparing capitalized words with non-capitalized words (i.e. 'whale' vs 'Whale')\n",
    "6. Finding the most frequently occurring (unique) tokens in the text and their frequency\n",
    "7. Specific characteristics in tokens\n",
    "8. The longest word in text its length\n",
    "9. Unique words have a frequency of more than 2000 and their frequency value\n",
    "10. The average number of tokens per sentence\n",
    "11. The 5 most frequent parts of speech in this text and their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# If you would like to work with the raw text you can use 'moby_raw'\n",
    "with open('Data/moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "    \n",
    "# If you would like to work with the novel in nltk. \n",
    "# Text format you can use 'text1'\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Total of tokens\n",
    "This is what you can do to fin the total tokens (words and punctuation symbols) within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255038, 255038)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.word_tokenize(moby_raw)),len(text1) # both methods are valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Total of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20742, 20742)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(nltk.word_tokenize(moby_raw))),len(set(text1)) # both methods are valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Change in unique tokens after lemmatizing the verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16887"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
    "\n",
    "len(set(lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lexical diversity of the text (i.e. ratio of unique tokens to the total number of tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08132905684643073"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(moby_tokens))/len(moby_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparing capitalized words with non-capitalized words (i.e. 'whale' vs 'Whale')\n",
    "\n",
    "Have a look at the [documentation](http://www.nltk.org/api/nltk.html?highlight=freqdist) of FreqDist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41248755087477157"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "    \n",
    "dist = FreqDist(moby_tokens)\n",
    "((dist[\"whale\"]+dist[\"Whale\"])/len(text1))*100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Finding the most frequently occurring (unique) tokens in the text and their frequency\n",
    "\n",
    "For example, the first 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7306),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978),\n",
       " ('his', 2459),\n",
       " ('it', 2196),\n",
       " ('I', 2113),\n",
       " ('!', 1767),\n",
       " ('is', 1722),\n",
       " ('--', 1713),\n",
       " ('with', 1659),\n",
       " ('he', 1658),\n",
       " ('was', 1639),\n",
       " ('as', 1620)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unique_tokens():\n",
    "    from nltk.probability import FreqDist\n",
    "    \n",
    "    x = dict(FreqDist(moby_tokens))\n",
    "    # this sorts the dictionary according to the keys.\n",
    "    dic = {k: v for k, v in sorted(x.items(), key=lambda item: item[1],reverse=True)}\n",
    "    \n",
    "    tokens = list(dic.keys())\n",
    "    freq = list(dic.values())\n",
    "    \n",
    "    return [(tokens[_],freq[_]) for _ in range(20)]\n",
    "\n",
    "unique_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Specific characteristics in tokens\n",
    "\n",
    "Fo example, let us fing the tokens that have a length of greater than 5 and frequency of more than 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain',\n",
       " 'Pequod',\n",
       " 'Queequeg',\n",
       " 'Starbuck',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'himself',\n",
       " 'little',\n",
       " 'seemed',\n",
       " 'should',\n",
       " 'though',\n",
       " 'through',\n",
       " 'whales',\n",
       " 'without']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "    \n",
    "dist = FreqDist(moby_tokens)\n",
    "vocab = dist.keys()\n",
    "freqwords = [w for w in vocab if len(w) > 5 and dist[w] > 150]\n",
    "    \n",
    "sorted(freqwords) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. The longest word in text its length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = max( len(x) for x in text1)\n",
    "w = [x for x in text1 if len(x) == l]\n",
    "(w[0],l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Unique words have a frequency of more than 2000 and their frequency value\n",
    "\n",
    "**Hint**: [`isalpha()`](https://www.geeksforgeeks.org/python-string-isalpha-application/) is used to check if the token is a word and not punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13715, 'the'),\n",
       " (6513, 'of'),\n",
       " (6010, 'and'),\n",
       " (4545, 'a'),\n",
       " (4515, 'to'),\n",
       " (3908, 'in'),\n",
       " (2978, 'that'),\n",
       " (2459, 'his'),\n",
       " (2196, 'it'),\n",
       " (2113, 'I')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unique_words_and_frequency():\n",
    "    from nltk.probability import FreqDist\n",
    "    import pandas as pd\n",
    "    \n",
    "    dist = FreqDist(moby_tokens)\n",
    "    vocab = dist.keys()\n",
    "    freq_words  = [w for w in vocab if dist[w] > 2000 and w.isalpha() == True]\n",
    "    frequencies = [ dict(dist)[_] for _ in freq_words]\n",
    "    \n",
    "    df = (pd.DataFrame([freq_words,frequencies])\n",
    "          .T.rename(columns={0:\"words\",1:\"freq\"})\n",
    "          .sort_values(by='freq', ascending=False)\n",
    "          .reset_index())\n",
    "    \n",
    "    return [(df[\"freq\"][_],df[\"words\"][_]) for _ in range(len(df))]\n",
    "\n",
    "unique_words_and_frequency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. The average number of tokens per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of tokens per sentence is:  26.0\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(moby_raw)\n",
    "num_tps = [ len(nltk.word_tokenize(s)) for s in sentences]\n",
    "    \n",
    "print(\"The average number of tokens per sentence is: \",round(sum(num_tps)/len(sentences),0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. The 5 most frequent parts of speech in this text and their frequency\n",
    "\n",
    "<img src=\"word_classes.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DT', 13715), ('IN', 6513), ('CC', 6010), ('DT', 4545), ('TO', 4515)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_frequency(top = 5):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    top -- integer: number to set the limit of firt values to display.\n",
    "    \n",
    "    Returns\n",
    "    pos_list -- list:  list of tuples of the form (part_of_speech, frequency) sorted \n",
    "                       in descending order of frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_tokens_frequency = unique_tokens()\n",
    "    \n",
    "    words_freq = [wf for wf in unique_tokens_frequency if wf[0].isalpha()==True]\n",
    "    POS = nltk.pos_tag([words_freq[w][0] for w in range(5)])\n",
    "    \n",
    "    pos_list = [(POS[i][1],words_freq[i][1]) for i in range(top)]\n",
    "\n",
    "    return pos_list\n",
    "\n",
    "pos_frequency()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "r35En",
   "launcher_item_id": "tCVfW",
   "part_id": "NTVgL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
